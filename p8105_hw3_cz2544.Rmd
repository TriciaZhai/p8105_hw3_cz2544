---
title: "p8105_hw3_cz2544"
author: "Chunxiao Zhai"
date: "10/12/2018"
output: github_document
---

```{r setup, include=FALSE}
library(p8105.datasets)
library(tidyverse)
library(ggridges)
library(viridis)
library(patchwork)
knitr::opts_chunk$set(
  echo = TRUE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_bw() + theme(legend.position = "bottom"))
```

#Problem 1
**1. Data cleaning**
Format the data to use appropriate variable names;
Focus on the “Overall Health” topic
Include only responses from “Excellent” to “Poor”
Organize responses as a factor taking levels ordered from “Excellent” to “Poor”
```{r data_clean}
brfss_tidy = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  select(year, locationabbr, locationdesc, topic, response, data_value, geo_location) %>% 
  filter(topic == "Overall Health") %>% 
  rename(prop = data_value) %>% 
  mutate(response = factor(response, levels = c("Excellent","Very good","Good","Fair","Poor" )))
```
**2. Using this dataset, do or answer the following (commenting on the results of each):**
**In 2002, which states were observed at 7 locations?**
```{r who_is_7}
brfss_tidy %>% 
  filter(year == 2002) %>% 
  group_by(locationabbr) %>% 
  distinct(geo_location) %>% 
  filter(n() == 7) %>%
  summarise()
```
**Comment:** In 2002, there were three states observed at 7 locations: Connecticut, Florida and North Carolina.

**Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.**
```{r number_of_each_place_plot}
p1 = brfss_tidy %>%
  group_by(locationabbr, year) %>% 
  mutate(n = n_distinct(geo_location)) %>% 
  # in case to check the outstanding state of "FL", unmute the filter
  # filter(year == 2007, n >= 40) %>% 
  ggplot(aes(x = year, y = n, color = locationabbr)) +
  geom_line(alpha = 0.7, size = 1) + 
  labs(
    title = "spaghetti plot of number of locations in each state",
    x = "year 2002-2008",
    y = "number of locations",
    caption = "Data from BRFSS"
  ) + 
  scale_x_continuous(breaks = c(2002,2003,2004,2005,2006,2007,2008),
                     limits = c(2002, 2008)) +
  viridis::scale_color_viridis(option="inferno", name = "States", discrete = TRUE) +
  theme(legend.direction = "horizontal", legend.key.size = unit(0.3, "cm"),
        legend.text = element_text(size = 6),
        legend.title = element_text(size = 6))
p1_s = p1 + guides(col = guide_legend(ncol = 26, nrow = 2, override.aes = list(size = 1.5)))
#ggsave("number_of_each_place_spaghetti_plot.pdf", p1_s, dpi = 600)
```
**Comment:**The number of locations in each state from 2002 to 2010 is generally stably below 10 for most states, but there are four states showed some increase and showed number between 10 and 20. In 2007, there is one state  showed significantly high number over 40 while its average number in all other years is below 10. After rechecking the data found it is Florida. Florida has 67 counties so the number is possible. In a spaghetti with over 50 variables, which color is what is just imposibble to distinguish, only outstanding values would be easier to recognize.

**Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.**
```{r table_new_york_3_year}
brfss_tidy %>%
  filter(year %in% c(2002, 2006, 2010), locationabbr == "NY", response == "Excellent") %>% 
  group_by(year) %>% 
  summarise(mean(prop), sd(prop)) %>% 
  knitr::kable(digits = 2)
```
**Comment:**The mean and standard deviation of the proportion of “Excellent” responses across locations in NY State is not very different in year 2002, 2006 and 2010. The year 2002 showed slightly higher mean but also higher standard deviation.

**For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.**
```{r response_proportion_plot}
p2 = brfss_tidy %>%
  group_by(year, locationabbr, response) %>% 
  mutate(mean = mean(prop)) %>% 
  distinct(year, locationabbr, mean) %>% 
  # in case to check the outstanding state "WV", unmute the filter
  #filter(locationabbr == "WV") %>% 
  distinct(year, locationabbr, mean) %>% 
  ggplot(aes(x = year, y = mean, color = locationabbr)) +
  geom_point(size = 1) +
  geom_line(alpha = 0.3, size = 0.3) +
  labs(
    title = "Distribution of state-level average proportions over time",
    x = "Year 2002-2008",
    y = "Average proportion %",
    caption = "Data from BRFSS"
  ) + 
  scale_x_continuous(breaks = c(2002,2003,2004,2005,2006,2007,2008),
                     limits = c(2002, 2008)) +
  viridis::scale_color_viridis(option="inferno", name = "States", discrete = TRUE) +
  theme(axis.text.x = element_text(angle =  -45,size = 7 ),
        axis.title.x = element_text(size = 9),
        legend.direction = "horizontal", legend.key.size = unit(0.3, "cm"),
        legend.text = element_text(size = 6),
        legend.title = element_text(size = 6))+
  facet_grid(~response) 
p2_s = p2 + guides(col = guide_legend(ncol = 26, nrow = 2, override.aes = list(size = 1.5)))
#ggsave("response_proportion_plot.pdf", p2_s, dpi = 600)
```
**Comment:**The diatribution of mean average proportion in each response category is different, with highest mean average proportion in "Very Good", mainly between 30-40%, then "Good", majority falled into 25-35%, then followed by "Ecxellent", fluctuate between 18-28%. The mean propotion of response "Fair" is 7-15%, and 2-6% for those who responsed "Poor", which showed narrower distribution than the "Good" to "Excellent". It is obvious that one state - "WV", in light lime, had lower average proportion of responses across years in the "Excellent", but higher in the "Poor". The "Overall Health" could be lower in this state due to this noticeable difference in the distribution of average proportion of different responses.

#Problem 2
```{r load_food_data}
data(instacart)
skimr::skim(instacart)
```
**1. Exploration of dataset**. 
Write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illstrative examples of observations. Then, do or answer the following (commenting on the results of each):

**Description**The dataset has 15 variables and 1384617 observations without missing values. The names have been cleaned with character variables of aisle, department, eval_set and product_name, integer variables of add_to_cart_order, aisle_id, days_since_prior_order, department_id, order_dow, order_hour_of_day, order_id, order_number, product_id, reordered and user_id. The variables can be put into 6 classes: 1.id to track a order/orderer (order, user)(id), 2. catergories and identification of a product (department, aisle, product)(name, id), 3. order time/location distribution (order_hour_of_day, order_dow, eval_set), 4. frequency of order (reorder, days_since_prior_order), 5. other character of user behavior (add_to_cart_order). And two variables I don not know what exactly meaning yet is (order_number).

It seems to be a dataset about shopping records of `r n_distinct(instacart$user_id)` users and their `r n_distinct(instacart$order_id)` orders, thus each user had made only one order during the dataset study period. 
There is a total `r n_distinct(instacart$product_id)` kinds of products, in `r n_distinct(instacart$department)` departments and `r n_distinct(instacart$aisle)` aisles. Products, aisles, and departments were recored with both name(character) and ID(integer). 
Through 0 to 23, orders are most often made at`r median(instacart$order_hour_of_day)`, mostly between`r quantile(instacart$order_hour_of_day,c(0.25, 0.75))`. Throuhgh day 0 to 6 in a week, orders are most often made at`r median(instacart$order_dow)`, mostly between`r quantile(instacart$order_dow, c(0.25, 0.75))`.  Train is the only location in eval_set. 
The average reorder rate is `r mean(instacart$reorder)`, with average days since prior order of `r mean(instacart$days_since_prior_order)`+/-`r sd(instacart$days_since_prior_order)`, the median is `r median(instacart$days_since_prior_order)`, range between `r quantile(instacart$days_since_prior_order, c(0,1))`.
The mean number of add_to_cart_order is `r mean(instacart$add_to_cart_order)`+/-`r sd(instacart$add_to_cart_order)`, and the median is `r median(instacart$add_to_cart_order)`, range between `r quantile(instacart$add_to_cart_order, c(0,1))`.
The mean number of order_number is `r mean(instacart$order_number)`+/-`r sd(instacart$order_number)`, and median is `r median(instacart$order_number)`, range between `r quantile(instacart$order_number, c(0,1))`.
```{r explore_most_popular}
instacart %>% 
  group_by(product_name) %>% 
  filter(n() > 10000) %>% 
  summarise(n = n(), mean(reordered), mean(days_since_prior_order)) %>% 
  knitr::kable(digits = 2)
instacart %>% 
  group_by(product_name) %>% 
  filter(n() %in% c(8000:10000) ) %>% 
  summarise(n = n(), mean(reordered), mean(days_since_prior_order)) %>% 
  knitr::kable(digits = 2)
```
The top 3 popular products are : Banana, Bag of Organic Bananas, and Organic Strawberries, all sold over 10000, with mean reorder rate of 0.88, 0.86, 0.79 and mean days since prior order for these products are 16.71, 15.74 and 15.45. The 4th and 5th most popular products are Large Lemon and Organic Baby Spinach, bought 9784 and 8135 times with mean reorder rate of 0.82, 0.73 and mean days since prior order 17.12, 17.58.

The following is a plot showing how products of different departments is distributed in different reorder rate and number of purchases. It may crash R so it won't run during the knitting.
```{r explore_popularity_reorder_diatribution, eval = FALSE}
p3 = instacart %>% 
  group_by(department) %>% 
  mutate(n = n(), re = mean(reordered), t = mean(days_since_prior_order)) %>% 
  ggplot(aes(x = n, y = re, color = department, alpha = 0.7)) +
  geom_point(size = 5) +
  scale_x_log10() +
  scale_y_continuous(labels = percent)
  labs(
    title = "distribution of popularity and reorder rate by department",
    x = "number of purchases",
    y = "reorder rate",
    caption = "Data from instacart"
  ) +
  viridis::scale_color_viridis( name = "department", discrete = TRUE) +
  theme(legend.direction = "horizontal", legend.key.size = unit(0.4, "cm"),
        legend.text = element_text(size = 7.5),
        legend.title = element_text(size = 8))
p3_s = p3 + guides(col = guide_legend(ncol = 11, nrow = 2, override.aes = list(size = 1.5)))
#ggsave("explore_popularity_reorder_diatribution_plot.pdf", p3_s, dpi = 150)
```

**2. Questions**
How many aisles are there, and which aisles are the most items ordered from?
```{r aisles_analysis}
instacart %>% 
  group_by(aisle) %>% 
  summarise(n= n()) %>%
  arrange(desc(n)) %>% 
  filter(row_number() <= 10) %>% 
  knitr::kable(digits = 2)
```
**Comment:** There are 134 aisles, most items are ordered from fresh vegetables, fresh fruits, packaged  vegetables fruits, yogurt, packaged cheese, water seltzer sparkling water, milk, chips pretzels, soy lactosefree and bread.

Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.
```{r plot_aisle_item_number}
aisle_rank =
  instacart %>% 
  group_by(aisle) %>% 
  summarise(n = n()) %>% 
  arrange((n)) %>%
  mutate(rank = row_number(), aisle_r = factor(aisle, levels = c(aisle))) 

p4 = aisle_rank %>% 
  ggplot(aes(x = rank, y = n, color = aisle_r)) +
  geom_point(size = 1.5) +
  scale_y_continuous(trans ='log10') +
  scale_x_discrete() +
  labs(
    title = "Number of items ordered by aisle",
    y = "Aisle",
    x = "Number of items",
    caption = "Data from instacart"
  ) +
  viridis::scale_color_viridis( name = "aisle_r", discrete = TRUE) +
  theme(legend.direction = "horizontal", 
        legend.key.size = unit(1, "mm"), legend.spacing = unit(-0.2, "mm"),
        plot.title = element_text(size = 6), plot.caption = element_text(size = 6),
        axis.title.x = element_text(size = 6), axis.title.y = element_text(size = 5),
        axis.text.y = element_text(size = 4))

p4_s = p4 + guides(col = guide_legend(ncol = 134, nrow = 1,
                                      label.position = "bottom", label.hjust = 0, label.vjust = 0, title.vjust = -0.5,
                                      label.theme = element_text(angle = -90, size = 3),
                                      title.theme = element_text(angle = -90, size = 0),
                                      lable.position = "bottom",
                                      override.aes = list(legend.position = "bottom", size = 0.48)))
ggsave("aisle_plot.pdf", p4_s, dpi = 600)
```
**Comment:**The range of number of items ordered in each aisle is very big, thus display the number in log transformation might make it easier to read. The size of plot and legend change at different rate while I zoom in and out, ggsave as pdf is a better way to guarantee reproducibility.

Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.
```{r table_popular_item_in_three_aisles}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  select(aisle, product_name) %>%  
  group_by(aisle, product_name) %>% 
  summarise(n = n()) %>% 
  ungroup() %>%
  group_by(aisle) %>% 
  filter(n == max(n)) %>% 
  knitr::kable(digits = 2)
```
**Comment:**The most popular item in baking ingredients is Light Brown Sugar, bought 499 times; The most popular item in dog food care is Snack Sticks Chicken & Rice Recipe Dog Treats bought 30 times, and The most popular item in packaged vegetables fruits is Organic Baby Spinach, bought 9784 times. As shown previously, there is huge differences in popularity of total purchases in different aisles, now find out the popularity difference in most popular items in different aisles is not a supprise. 

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).
```{r table_hour_Apples_Coffee_Ice_Cream}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples","Coffee Ice Cream")) %>% 
  group_by(aisle) %>% 
  summarise(n= n()) %>%
  arrange(n) %>% 
  mutate(rank = row_number()) %>% 
filter(row_number() <= 10) %>% 
  knitr::kable(digits = 2)
```

**Comment:**



#Problem 3
```{r}
data("ny_noaa")
```
The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and indicating the extent to which missing data is an issue. Then, do or answer the following (commenting on the results of each):

Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?
Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?
Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.



## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
